{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to cleaning steps\n",
    "1. Remove `'[deleted]'`\n",
    "2. Set datatypes for imports\n",
    "3. Review `flair` columns for refinement or subsetting\n",
    "4. Clean up `is_video`\n",
    "5. Process `media` column to extract author name, link, other content for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_posts_list = []\n",
    "# for file in os.listdir('./Data/posts/cleaned/'):\n",
    "#     all_posts_list.append(pd.read_csv(f'./Data/posts/cleaned/{file}'))\n",
    "\n",
    "# columns_ref = ['subreddit', 'subreddit_id', 'subreddit_type', 'id', 'name', 'media',\n",
    "#        'is_video', 'created_utc', 'num_comments', 'score', 'ups', 'selftext',\n",
    "#        'author_flair_text', 'link_flair_text', 'poll_data', 'created']\n",
    "\n",
    "# all_posts = pd.DataFrame(columns=columns_ref)\n",
    "# for file in all_posts_list:\n",
    "#     all_posts = pd.concat([all_posts, file.fillna('')])\n",
    "\n",
    "deleted_posts = all_posts[(all_posts['selftext']=='[deleted]')|(all_posts['selftext']=='[removed]')].index\n",
    "# complete_posts = all_posts.drop(index=deleted_posts)\n",
    "\n",
    "# len(all_posts)\n",
    "\n",
    "# sentiment_data.to_csv('./Data/posts/cleaned/sentiment_data.csv', index=False)\n",
    "# sentiment_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577969 113287 59825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "Depop          31.0\n",
       "Etsy           25.0\n",
       "Flipping       22.0\n",
       "poshmark        9.0\n",
       "EtsySellers     8.0\n",
       "stockx          3.0\n",
       "Grailed         3.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data = complete_posts[(complete_posts['media']=='')&(complete_posts['selftext']!='')][['subreddit','id','selftext']]\n",
    "# sentiment_data = sentiment_data[sentiment_data['media']=='']\n",
    "print(len(all_posts), len(complete_posts), len(sentiment_data))\n",
    "sentiment_data['subreddit'].value_counts(normalize=True).round(2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removed deleted & media-only posts, duplicates.\n",
    "Resulted in 59,825 unique posts.\n",
    "\n",
    "| site | percent |\n",
    "|--|--|\n",
    "| depop | 31% |\n",
    "| etsy | 25% |\n",
    "| flipping | 22% |\n",
    "| poshmark | 9% |\n",
    "| etsySellers | 8% |\n",
    "| stockx | 3% |\n",
    "| grailed | 3% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentiment_data = pd.read_csv('./Data/posts/cleaned/sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "all_stopwords = stopwords.words('english') +['anyone','someone','everyone','everybody','also','you','your','etsy','depop','grailed','ebay',\n",
    "                                             'http','www','com','reddit']\n",
    "# come back and make feature that notes if the post mentions another site that's not the sub\n",
    "\n",
    "def post_processor(sentence: str, tokenizer, lemmatizer, stopwords) -> str:\n",
    "    tokenized = tokenizer.tokenize(sentence.lower())\n",
    "    lemmatized = [lemmatizer.lemmatize(i) for i in tokenized]\n",
    "    no_stopwords = [w for w in lemmatized if w not in all_stopwords]\n",
    "    return ' '.join(no_stopwords)\n",
    "    \n",
    "j = 0\n",
    "total_posts = len(sentiment_data)\n",
    "cleaned_posts = []\n",
    "for post in sentiment_data['selftext']:\n",
    "    cleaned_posts.append(post_processor(post, tokenizer, lemmatizer, all_stopwords))\n",
    "    \n",
    "    if j+1 % 10_000 == 0:\n",
    "        print(j/total_posts)\n",
    "    \n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vectorizer.fit_transform(cleaned_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item        38865\n",
       "wa          37722\n",
       "like        22997\n",
       "get         21755\n",
       "seller      21308\n",
       "would       21227\n",
       "shipping    18304\n",
       "know        18270\n",
       "shop        18007\n",
       "one         17118\n",
       "time        17083\n",
       "ha          16869\n",
       "sale        16409\n",
       "amp         16366\n",
       "buyer       15328\n",
       "want        14553\n",
       "day         14464\n",
       "new         14437\n",
       "listing     13958\n",
       "sell        13610\n",
       "dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(output.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.dropna(subset='selftext', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    'buyer','customer',\n",
    "    'item','product',\n",
    "    'sale','sell','selling','sold','purchase','refund','return'\n",
    "    'money','paypal','offer','price','free','pay','fee','cost',\n",
    "    'listing','post','order',\n",
    "    'flipping',\n",
    "    'account','review','store','shop','business',\n",
    "    'photo',\n",
    "    'shipping','ship','sent','package','shipped','label','tracking','send',\n",
    "    'received',\n",
    "    'case', 'issue', # perhaps contested shipments or returns?\n",
    "    'advice','look','looking','search'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
