{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Unzip](#unzipping): **start here if first time running locally** unzipping historic data, converting and exporting as `.csv`   \n",
    "1. [Import](#csv-imports): **start here if `.csv` files are locally available** Importing data as `.csv` files   \n",
    "1. [Initial cleaning](#initial-cleaning): filling nulls, removing deleted records and duplicates, exporting `.csv` files    \n",
    "1. [Datatype management](#datatype-management): converting datatypes, setting up map for imports   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "from cleaning_assets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/stats/comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"unzipping\"></a>\n",
    "\n",
    "### Unzipping & transforming original files from [The Eye](https://the-eye.eu/redarcs/)\n",
    "#### `.zst` file processing & `.csv` transformation\n",
    "Skip to [next section](#csv-imports) if `.csv` files are locally available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long run time - only execute if starting from beginning, otherwise skip to import csv\n",
    "# Reading in files\n",
    "for file in os.listdir('./Data/original'):\n",
    "    file_name = file[:-4].lower()\n",
    "    site_name = file.split('_')[0].lower()\n",
    "    \n",
    "    # unzipper put together by Tamir Cohen\n",
    "    with open(f'./Data/original/{file}', 'rb') as fh:\n",
    "            # decompress\n",
    "        dctx = zstd.ZstdDecompressor(max_window_size=2_147_483_648)\n",
    "        stream_reader = dctx.stream_reader(fh)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        data = [] \n",
    "            # parse JSONs & add rows\n",
    "        for line in text_stream:\n",
    "            data.append(json.loads(line))\n",
    "        # store to correct container\n",
    "    if 'comments' in file_name:\n",
    "        all_comments[site_name] = pd.DataFrame(data)\n",
    "        print(f'{file_name} unzipped, containing {len(all_posts[site_name])} records')\n",
    "    elif 'submissions' in file_name:\n",
    "        all_posts[site_name] = pd.DataFrame(data)\n",
    "        print(f'{file_name} unzipped, containing {len(all_posts[site_name])} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for subfolder, if not create\n",
    "if not os.path.exists(f\"./Data/comments/csv/\"):\n",
    "    os.makedirs(\"./Data/comments/csv\") \n",
    "if not os.path.exists(f\"./Data/posts/csv/\"):\n",
    "    os.makedirs(\"./Data/posts/csv\") \n",
    "# exporting CSV\n",
    "for i in all_comments.keys():\n",
    "    all_comments[i].to_csv(f'./Data/comments/csv/{i}_comments.csv', index=False)\n",
    "    print(f'{i} saved')\n",
    "    \n",
    "for i in all_posts.keys():\n",
    "    all_posts[i].to_csv(f'./Data/posts/csv/{i.lower()}_posts.csv', index=False)\n",
    "    print(f'{i} saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"csv-imports\"></a>\n",
    "\n",
    "### Importing Files\n",
    "#### Run from here if `csv` files are locally available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./Data/comments/csv'):\n",
    "    file_name = file[:-4]\n",
    "    site_name = file.split('_')[0]\n",
    "    all_comments[site_name] = pd.read_csv(f'./Data/comments/csv/{file}', low_memory=False)\n",
    "    \n",
    "for file in os.listdir('./Data/posts/csv'):\n",
    "    file_name = file[:-4]\n",
    "    site_name = file.split('_')[0]\n",
    "    all_posts[site_name] = pd.read_csv(f'./Data/posts/csv/{file}', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"initial-cleaning\"></a>\n",
    "\n",
    "### Initial Cleaning\n",
    "- filling nulls with blanks (numeric data later filled with `-1` in [datatype management](#datatype-management))\n",
    "- dropped `removed` and `deleted` records\n",
    "- dropped full-row duplicates and text body duplicates*\n",
    "- generated metadata  \n",
    "   \n",
    "_*for this analysis, the content is more important than time of post and frequency. For other areas of analysis, consider leaving in text body duplicates._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depop\n",
      "    810,794 records to 718,825 with 378,672 linked posts.\n",
      "    91,969 duplicate comments removed.\n",
      "etsysellers\n",
      "    339,569 records to 310,249 with 162,599 linked posts.\n",
      "    29,320 duplicate comments removed.\n",
      "etsy\n",
      "    662,752 records to 603,159 with 303,311 linked posts.\n",
      "    59,593 duplicate comments removed.\n",
      "flipping\n",
      "    1,591,386 records to 1,475,464 with 773,985 linked posts.\n",
      "    115,922 duplicate comments removed.\n",
      "grailed\n",
      "    55,955 records to 43,363 with 28,159 linked posts.\n",
      "    12,592 duplicate comments removed.\n",
      "poshmark\n",
      "    566,272 records to 467,946 with 227,477 linked posts.\n",
      "    98,326 duplicate comments removed.\n",
      "stockx\n",
      "    140,024 records to 126,593 with 75,929 linked posts.\n",
      "    13,431 duplicate comments removed.\n",
      "    columns not available: ['ups']\n"
     ]
    }
   ],
   "source": [
    "# print('-----comments-----')\n",
    "for site, dirty_file in all_comments.items():\n",
    "    columns = [i for i in comments_cols if i in dirty_file.columns]\n",
    "    file = dirty_file[columns].copy()\n",
    "    removed_records = file[\n",
    "        (file['body']=='[deleted]')|\n",
    "        (file['body']=='[removed]')|\n",
    "        (file['body'].str.contains('comment.+removed'))].index\n",
    "    \n",
    "    file.drop(index=removed_records, inplace=True)\n",
    "    file.drop_duplicates(inplace=True)\n",
    "    file.drop_duplicates(subset=['body'], inplace=True)\n",
    "    original_length, new_length, unique_posts = len(dirty_file), len(file), len(file['parent_id'].unique())\n",
    "\n",
    "    comments_cleaned[site] = file\n",
    "    comments_full.append(original_length)\n",
    "    comments_unique.append(new_length)\n",
    "    comments_pct_removed.append((original_length-new_length)/original_length)\n",
    "    unique_linked_posts.append(unique_posts)\n",
    "\n",
    "    print(f'''{site}\n",
    "    {original_length:,} records to {new_length:,} with {unique_posts:,} linked posts.\n",
    "    {original_length-new_length:,} duplicate comments removed.''')\n",
    "    if [i for i in comments_cols if i not in file.columns]:\n",
    "        print(f'    columns not available: {[i for i in comments_cols if i not in file.columns]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depop\n",
      "    126,523 records to 39,167.\n",
      "    87,356 duplicate posts removed.\n",
      "etsysellers\n",
      "    38,292 records to 24,978.\n",
      "    13,314 duplicate posts removed.\n",
      "etsy\n",
      "    98,967 records to 43,270.\n",
      "    55,697 duplicate posts removed.\n",
      "    columns not available: ['poll_data']\n",
      "flipping\n",
      "    92,907 records to 42,310.\n",
      "    50,597 duplicate posts removed.\n",
      "grailed\n",
      "    20,449 records to 4,881.\n",
      "    15,568 duplicate posts removed.\n",
      "poshmark\n",
      "    57,893 records to 22,881.\n",
      "    35,012 duplicate posts removed.\n",
      "stockx\n",
      "    23,288 records to 10,321.\n",
      "    12,967 duplicate posts removed.\n"
     ]
    }
   ],
   "source": [
    "# print('-----posts-----')\n",
    "for site, dirty_file in all_posts.items():\n",
    "    columns = [i for i in posts_cols if i in dirty_file.columns]\n",
    "    file = dirty_file[columns].copy()\n",
    "    removed_records = file[\n",
    "        (file['selftext']=='[deleted]')|\n",
    "        (file['selftext']=='[removed]')|\n",
    "        (file['selftext'].str.contains('post.+removed'))].index\n",
    "    \n",
    "    file.drop(index=removed_records, inplace=True)\n",
    "    file.drop_duplicates(inplace=True)\n",
    "    file.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "    original_length, new_length = len(dirty_file), len(file)\n",
    "        \n",
    "    posts_cleaned[site] = file\n",
    "    posts_full.append(original_length)\n",
    "    posts_unique.append(new_length)\n",
    "    posts_pct_removed.append((original_length-new_length)/original_length)\n",
    "    \n",
    "    print(f'''{site}\n",
    "    {original_length:,} records to {new_length:,}.\n",
    "    {original_length-new_length:,} duplicate posts removed.''')\n",
    "    if [i for i in posts_cols if i not in file.columns]:\n",
    "        print(f'    columns not available: {[i for i in posts_cols if i not in file.columns]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_log = pd.DataFrame(zip(all_comments.keys(), \n",
    "                                comments_full, \n",
    "                                comments_unique,\n",
    "                                unique_linked_posts,\n",
    "                                comments_pct_removed), \n",
    "                            columns=['site','total records','unique records','unique linked posts','pct removed'])\n",
    "# print(f\"On average, {round(comments_log['pct removed'].mean(),4)*100}% of records removed\")\n",
    "# comments_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_log = pd.DataFrame(zip(all_comments.keys(), \n",
    "                             posts_full, \n",
    "                             posts_unique, \n",
    "                             posts_pct_removed), \n",
    "                         columns=['site','total records','unique records','pct removed'])\n",
    "# print(f\"On average, {round(posts_log['pct removed'].mean(),4)*100}% of records removed\")\n",
    "# posts_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"datatype-management\"></a>\n",
    "\n",
    "### Setting and converting datatypes\n",
    "#### Posts\n",
    "- Convert numeric columns to `int` & fill nulls with `-1`: `created_utc`, `num_comments`, `score`, `ups`\n",
    "- Convert binary to `bool`: `is_video`\n",
    "- Generate timestamp `created` using `created_utc`\n",
    "\n",
    "#### Comments\n",
    "- Convert numeric columns to `int` & fill nulls with `-1`: `created_utc`, `score`, `ups`, `likes`\n",
    "- Convert binary to `bool`: `controversiality`\n",
    "- Generate timestamp `created` using `created_utc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, file in posts_cleaned.items():\n",
    "    for column, type in posts_cols_convert.items():\n",
    "        file[column] = file[column].fillna(-1).astype(type)    \n",
    "    file['created'] = file['created_utc'].apply(datetime.fromtimestamp)\n",
    "    \n",
    "for site, file in comments_cleaned.items():\n",
    "    for column, type in {key: value for key, value in comments_cols_convert.items() if key in file.columns}.items():\n",
    "        file[column] = file[column].fillna(-1).astype(type)   \n",
    "    file['created'] = file['created_utc'].apply(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_cleaned['depop'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in posts_cleaned.values():\n",
    "    start_dates.append(datetime.date(file['created'].min()))\n",
    "    end_dates.append(datetime.date(file['created'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depop-cleaned saved\n",
      "etsysellers-cleaned saved\n",
      "etsy-cleaned saved\n",
      "flipping-cleaned saved\n",
      "grailed-cleaned saved\n",
      "poshmark-cleaned saved\n",
      "stockx-cleaned saved\n",
      "depop-cleaned saved\n",
      "etsysellers-cleaned saved\n",
      "etsy-cleaned saved\n",
      "flipping-cleaned saved\n",
      "grailed-cleaned saved\n",
      "poshmark-cleaned saved\n",
      "stockx-cleaned saved\n"
     ]
    }
   ],
   "source": [
    "# checking for subfolder, if not create\n",
    "if not os.path.exists(f\"./Data/comments/cleaned/\"):\n",
    "    os.makedirs(\"./Data/comments/cleaned/\") \n",
    "if not os.path.exists(f\"./Data/posts/cleaned/\"):\n",
    "    os.makedirs(\"./Data/posts/cleaned/\") \n",
    "# exporting CSV\n",
    "for site, file in comments_cleaned.items():\n",
    "    file.to_csv(f'./Data/comments/cleaned/{site}_comments.csv', index=False)\n",
    "    print(f'{site}-cleaned saved')\n",
    "    \n",
    "for site, file in posts_cleaned.items():\n",
    "    file.to_csv(f'./Data/posts/cleaned/{site}_posts.csv', index=False)\n",
    "    print(f'{site}-cleaned saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_log['start date'] = start_dates\n",
    "posts_log['end date'] = end_dates\n",
    "# posts_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./Data/stats/\"):\n",
    "    os.makedirs(\"./Data/stats/\") \n",
    "\n",
    "comments_log.to_csv('./Data/stats/comments.csv', index=False)\n",
    "posts_log.to_csv('./Data/stats/posts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
